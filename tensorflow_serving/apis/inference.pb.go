// Code generated by protoc-gen-go. DO NOT EDIT.
// source: github.com/smoug25/tensorflow_serving_helper/tensorflow_serving/apis/inference.proto

package apis

import (
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

// Inference request such as classification, regression, etc...
type InferenceTask struct {
	// Model Specification. If version is not specified, will use the latest
	// (numerical) version.
	// All ModelSpecs in a MultiInferenceRequest must access the same model name.
	ModelSpec *ModelSpec `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec,proto3" json:"model_spec,omitempty"`
	// Signature's method_name. Should be one of the method names defined in
	// third_party/tensorflow/python/saved_model/signature_constants.py.
	// e.g. "tensorflow/serving/classify".
	MethodName           string   `protobuf:"bytes,2,opt,name=method_name,json=methodName,proto3" json:"method_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferenceTask) Reset()         { *m = InferenceTask{} }
func (m *InferenceTask) String() string { return proto.CompactTextString(m) }
func (*InferenceTask) ProtoMessage()    {}
func (*InferenceTask) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e91ac43fb23204b, []int{0}
}

func (m *InferenceTask) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferenceTask.Unmarshal(m, b)
}
func (m *InferenceTask) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferenceTask.Marshal(b, m, deterministic)
}
func (m *InferenceTask) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferenceTask.Merge(m, src)
}
func (m *InferenceTask) XXX_Size() int {
	return xxx_messageInfo_InferenceTask.Size(m)
}
func (m *InferenceTask) XXX_DiscardUnknown() {
	xxx_messageInfo_InferenceTask.DiscardUnknown(m)
}

var xxx_messageInfo_InferenceTask proto.InternalMessageInfo

func (m *InferenceTask) GetModelSpec() *ModelSpec {
	if m != nil {
		return m.ModelSpec
	}
	return nil
}

func (m *InferenceTask) GetMethodName() string {
	if m != nil {
		return m.MethodName
	}
	return ""
}

// Inference result, matches the type of request or is an error.
type InferenceResult struct {
	ModelSpec *ModelSpec `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec,proto3" json:"model_spec,omitempty"`
	// Types that are valid to be assigned to Result:
	//	*InferenceResult_ClassificationResult
	//	*InferenceResult_RegressionResult
	Result               isInferenceResult_Result `protobuf_oneof:"result"`
	XXX_NoUnkeyedLiteral struct{}                 `json:"-"`
	XXX_unrecognized     []byte                   `json:"-"`
	XXX_sizecache        int32                    `json:"-"`
}

func (m *InferenceResult) Reset()         { *m = InferenceResult{} }
func (m *InferenceResult) String() string { return proto.CompactTextString(m) }
func (*InferenceResult) ProtoMessage()    {}
func (*InferenceResult) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e91ac43fb23204b, []int{1}
}

func (m *InferenceResult) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferenceResult.Unmarshal(m, b)
}
func (m *InferenceResult) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferenceResult.Marshal(b, m, deterministic)
}
func (m *InferenceResult) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferenceResult.Merge(m, src)
}
func (m *InferenceResult) XXX_Size() int {
	return xxx_messageInfo_InferenceResult.Size(m)
}
func (m *InferenceResult) XXX_DiscardUnknown() {
	xxx_messageInfo_InferenceResult.DiscardUnknown(m)
}

var xxx_messageInfo_InferenceResult proto.InternalMessageInfo

func (m *InferenceResult) GetModelSpec() *ModelSpec {
	if m != nil {
		return m.ModelSpec
	}
	return nil
}

type isInferenceResult_Result interface {
	isInferenceResult_Result()
}

type InferenceResult_ClassificationResult struct {
	ClassificationResult *ClassificationResult `protobuf:"bytes,2,opt,name=classification_result,json=classificationResult,proto3,oneof"`
}

type InferenceResult_RegressionResult struct {
	RegressionResult *RegressionResult `protobuf:"bytes,3,opt,name=regression_result,json=regressionResult,proto3,oneof"`
}

func (*InferenceResult_ClassificationResult) isInferenceResult_Result() {}

func (*InferenceResult_RegressionResult) isInferenceResult_Result() {}

func (m *InferenceResult) GetResult() isInferenceResult_Result {
	if m != nil {
		return m.Result
	}
	return nil
}

func (m *InferenceResult) GetClassificationResult() *ClassificationResult {
	if x, ok := m.GetResult().(*InferenceResult_ClassificationResult); ok {
		return x.ClassificationResult
	}
	return nil
}

func (m *InferenceResult) GetRegressionResult() *RegressionResult {
	if x, ok := m.GetResult().(*InferenceResult_RegressionResult); ok {
		return x.RegressionResult
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*InferenceResult) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*InferenceResult_ClassificationResult)(nil),
		(*InferenceResult_RegressionResult)(nil),
	}
}

// Inference request containing one or more requests.
type MultiInferenceRequest struct {
	// Inference tasks.
	Tasks []*InferenceTask `protobuf:"bytes,1,rep,name=tasks,proto3" json:"tasks,omitempty"`
	// Input data.
	Input                *Input   `protobuf:"bytes,2,opt,name=input,proto3" json:"input,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *MultiInferenceRequest) Reset()         { *m = MultiInferenceRequest{} }
func (m *MultiInferenceRequest) String() string { return proto.CompactTextString(m) }
func (*MultiInferenceRequest) ProtoMessage()    {}
func (*MultiInferenceRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e91ac43fb23204b, []int{2}
}

func (m *MultiInferenceRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_MultiInferenceRequest.Unmarshal(m, b)
}
func (m *MultiInferenceRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_MultiInferenceRequest.Marshal(b, m, deterministic)
}
func (m *MultiInferenceRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MultiInferenceRequest.Merge(m, src)
}
func (m *MultiInferenceRequest) XXX_Size() int {
	return xxx_messageInfo_MultiInferenceRequest.Size(m)
}
func (m *MultiInferenceRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_MultiInferenceRequest.DiscardUnknown(m)
}

var xxx_messageInfo_MultiInferenceRequest proto.InternalMessageInfo

func (m *MultiInferenceRequest) GetTasks() []*InferenceTask {
	if m != nil {
		return m.Tasks
	}
	return nil
}

func (m *MultiInferenceRequest) GetInput() *Input {
	if m != nil {
		return m.Input
	}
	return nil
}

// Inference request containing one or more responses.
type MultiInferenceResponse struct {
	// List of results; one for each InferenceTask in the request, returned in the
	// same order as the request.
	Results              []*InferenceResult `protobuf:"bytes,1,rep,name=results,proto3" json:"results,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *MultiInferenceResponse) Reset()         { *m = MultiInferenceResponse{} }
func (m *MultiInferenceResponse) String() string { return proto.CompactTextString(m) }
func (*MultiInferenceResponse) ProtoMessage()    {}
func (*MultiInferenceResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e91ac43fb23204b, []int{3}
}

func (m *MultiInferenceResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_MultiInferenceResponse.Unmarshal(m, b)
}
func (m *MultiInferenceResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_MultiInferenceResponse.Marshal(b, m, deterministic)
}
func (m *MultiInferenceResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_MultiInferenceResponse.Merge(m, src)
}
func (m *MultiInferenceResponse) XXX_Size() int {
	return xxx_messageInfo_MultiInferenceResponse.Size(m)
}
func (m *MultiInferenceResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_MultiInferenceResponse.DiscardUnknown(m)
}

var xxx_messageInfo_MultiInferenceResponse proto.InternalMessageInfo

func (m *MultiInferenceResponse) GetResults() []*InferenceResult {
	if m != nil {
		return m.Results
	}
	return nil
}

func init() {
	proto.RegisterType((*InferenceTask)(nil), "tensorflow.serving.InferenceTask")
	proto.RegisterType((*InferenceResult)(nil), "tensorflow.serving.InferenceResult")
	proto.RegisterType((*MultiInferenceRequest)(nil), "tensorflow.serving.MultiInferenceRequest")
	proto.RegisterType((*MultiInferenceResponse)(nil), "tensorflow.serving.MultiInferenceResponse")
}

func init() {
	proto.RegisterFile("github.com/smoug25/tensorflow_serving_helper/tensorflow_serving/apis/inference.proto", fileDescriptor_1e91ac43fb23204b)
}

var fileDescriptor_1e91ac43fb23204b = []byte{
	// 390 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xac, 0x92, 0x4f, 0xaf, 0xd2, 0x40,
	0x14, 0xc5, 0x2d, 0x04, 0x94, 0x69, 0x8c, 0x3a, 0x11, 0x83, 0x24, 0x46, 0xac, 0x2e, 0xba, 0x6a,
	0x93, 0x1a, 0xe3, 0x46, 0x37, 0xe8, 0x42, 0x16, 0x18, 0x33, 0x60, 0x8c, 0x6e, 0x9a, 0x52, 0x2e,
	0x65, 0x42, 0x3b, 0x33, 0xce, 0x9d, 0xea, 0xda, 0x4f, 0xe0, 0xd7, 0x75, 0x69, 0xec, 0xd0, 0x22,
	0xbc, 0xe6, 0x6d, 0x5e, 0x77, 0xcd, 0xf4, 0xdc, 0xdf, 0x39, 0xf7, 0x0f, 0x59, 0x67, 0xdc, 0xec,
	0xcb, 0x4d, 0x90, 0xca, 0x22, 0xc4, 0x42, 0x96, 0x59, 0xf4, 0x2a, 0x34, 0x20, 0x50, 0xea, 0x5d,
	0x2e, 0x7f, 0xc6, 0x08, 0xfa, 0x07, 0x17, 0x59, 0xbc, 0x87, 0x5c, 0x81, 0x6e, 0xf9, 0x13, 0x26,
	0x8a, 0x63, 0xc8, 0xc5, 0x0e, 0x34, 0x88, 0x14, 0x02, 0xa5, 0xa5, 0x91, 0x94, 0x9e, 0x84, 0xc1,
	0x51, 0x38, 0xfd, 0xda, 0x89, 0x53, 0x9a, 0x27, 0x88, 0x7c, 0xc7, 0xd3, 0xc4, 0x70, 0x29, 0xac,
	0xdd, 0xf4, 0x53, 0x47, 0x4d, 0xa8, 0xd2, 0x74, 0x4a, 0x2c, 0xe4, 0x16, 0xf2, 0x23, 0xf1, 0x73,
	0x27, 0x44, 0x0d, 0x99, 0x06, 0xc4, 0xa6, 0x75, 0x4f, 0x90, 0xbb, 0x8b, 0x7a, 0xf8, 0xeb, 0x04,
	0x0f, 0xf4, 0x0d, 0x21, 0x95, 0x6d, 0x8c, 0x0a, 0xd2, 0x89, 0x33, 0x73, 0x7c, 0x37, 0x7a, 0x12,
	0x5c, 0xdd, 0x47, 0xb0, 0xfc, 0xa7, 0x5a, 0x29, 0x48, 0xd9, 0xa8, 0xa8, 0x3f, 0xe9, 0x53, 0xe2,
	0x16, 0x60, 0xf6, 0x72, 0x1b, 0x8b, 0xa4, 0x80, 0x49, 0x6f, 0xe6, 0xf8, 0x23, 0x46, 0xec, 0xd3,
	0xc7, 0xa4, 0x00, 0xef, 0x77, 0x8f, 0xdc, 0x6b, 0x0c, 0x19, 0x60, 0x99, 0x9b, 0x1b, 0x5a, 0xc6,
	0x64, 0x7c, 0xbe, 0xd4, 0x58, 0x57, 0xd8, 0xca, 0xdc, 0x8d, 0xfc, 0x36, 0xd0, 0xbb, 0xb3, 0x02,
	0x1b, 0xe3, 0xc3, 0x2d, 0xf6, 0x30, 0x6d, 0x79, 0xa7, 0x2b, 0xf2, 0xe0, 0x34, 0xb6, 0x1a, 0xde,
	0xaf, 0xe0, 0x2f, 0xda, 0xe0, 0xac, 0x11, 0x37, 0xe0, 0xfb, 0xfa, 0xe2, 0x6d, 0x7e, 0x87, 0x0c,
	0x2d, 0xc9, 0xfb, 0xe5, 0x90, 0xf1, 0xb2, 0xcc, 0x0d, 0xff, 0x6f, 0x2c, 0xdf, 0x4b, 0x40, 0x43,
	0x5f, 0x93, 0x81, 0x49, 0xf0, 0x80, 0x13, 0x67, 0xd6, 0xf7, 0xdd, 0xe8, 0x59, 0x9b, 0xd9, 0xd9,
	0xf2, 0x98, 0xd5, 0xd3, 0x90, 0x0c, 0xaa, 0x63, 0x3c, 0x8e, 0xe0, 0x71, 0x7b, 0xa1, 0x2a, 0x0d,
	0xb3, 0x3a, 0xef, 0x0b, 0x79, 0x74, 0x19, 0x01, 0x95, 0x14, 0x08, 0xf4, 0x2d, 0xb9, 0x6d, 0x73,
	0xd6, 0x29, 0x9e, 0x5f, 0x9b, 0xc2, 0x76, 0xc7, 0xea, 0x9a, 0xf9, 0xe2, 0xdb, 0xfb, 0x2e, 0xee,
	0xf6, 0x8f, 0xe3, 0x6c, 0x86, 0xd5, 0xc1, 0xbe, 0xfc, 0x1b, 0x00, 0x00, 0xff, 0xff, 0xe9, 0x90,
	0x10, 0x25, 0x72, 0x04, 0x00, 0x00,
}
